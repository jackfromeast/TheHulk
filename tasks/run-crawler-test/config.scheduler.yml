scheduler:
  TEST_NAME: "crawler-test"
  # Path to workspace (base directory for storing results)
  WORKSPACE: "/home/jackfromeast/Desktop/TheHulk/tasks/run-crawler-test/outputs"

  # Option: "seed" or "list"
  MODE: "list"
  SEED_URL: "https://baidu.com/index.html"
  URL_LIST: "/home/jackfromeast/Desktop/TheHulk/dataset/Trenco_KJX3W.csv"
  URL_LIST_FROM: 0
  URL_LIST_TO: 500


  # Format of the urls.csv file:
  #   1,google.com
  #   2,facebook.com
  #   3,amazonaws.com
  #   4,microsoft.com
  #   5,apple.com
  #   6,googleapis.com
  #   7,akamaiedge.net
  #   8,youtube.com
  #   9,a-msedge.net
  #   10,twitter.com  

  # Maximum number of workers to run in parallel
  MAX_WORKER: 8
  # Maximum number of urls to visit
  MAX_URL: 1
  # Time budget for each worker to visit a single domain
  TIMEOUT_PER_DOMAIN: 5400000


